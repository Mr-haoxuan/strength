# 防爬虫简述

爬虫会造成的问题主要分为两类。

1. 对价格和数据敏感的企业如果被竞争企业爬取数据可能会造成一定的损失。例如链家的网站，需要防止竞争对手爬取自己的标价。
2. 爬虫可能对于正常用户造成干扰，如果爬虫大量访问网站核心接口可能导致服务器过载，干扰正常用户访问。

**防爬虫的目的，是尽量减少对正常用户干扰的情况下尽可能增加有害爬虫访问的成本。**

在爬虫这个大命题下也可以对爬虫的种类和特点进行细分

1. 正常爬虫，例如搜索引擎的爬取。此类爬虫频率不高通常一天甚至几天才会访问一次，而且会较为严格遵守 robots.txt 文档中的内容进行数据收集，在爬取时会写明自己的 user-agent。在一些特殊情况下也会造成一些告警问题，但通常不会造成很大的危害。
2. 数据收集型的爬虫，该类爬虫主要收集网站数据，速度通常不高，一般一天一次到几次。但是该类爬虫有可能不遵循 robots 文档的内容爬取敏感信息。有较大的几率造成网站告警和信息流失。
3. 速度敏感型爬虫，该类爬虫的行为涵盖收集网站信息到网络攻击。例如黄牛爬虫刷票、频繁调用核心接口导致网站崩溃，危害最大。

# 对于爬虫的处理

针对以上对于爬虫的分类，以及我们自身技术体系的不同，需要不同的处理方法。

## 正常爬虫

对于该类型的爬虫，我们往往需要采取配合或者不处理的态度。主要有如下的方案。

1. 第一步永远是要写好 robots.txt 文档。例如：

```
// 允许所有的 user-agent 访问
User-agent: *

// 允许爬取根目录下的内容
Allow: /

// 禁止爬取 /wep-admin/ 下所有的资源
Disallow: /wp-admin/

// 禁止爬取 /wp-content/plugins/ 下所有的资源
Disallow: /wp-content/plugins/
```

### 未引发问题时

1.  对于静态的 html 页面可以不用处理，不会产生问题。
2. 对于实时调用接口加载数据的页面，如果你期望你的网站被搜索引擎收集。可以使用 SSR 等方式让引擎可以快速看到你想展示的信息。

### 引发问题时

例如你在前端代码中使用的 window 对象但爬虫环境可能并没有该对象造成了报错告警。

如何解决？

1. 最常用的方案就是通过 user-agent 进行区分处理。
2. 通过 robots.txt 处理。但是因为 robots.txt 是进行整体的屏蔽，所以可能会造成一些爬取信息的缺失。

## 数据收集型爬虫

数据收集类型的爬虫主要可能造成的是私密信息流失。对于此类爬虫通常在前端有如下防范方法。

### 针对爬虫大多没有视图层的原理

1. 将敏感数据进行分化处理。例如一个商品的价格，实际的价格是 123 而后端返回的是 231 ，前端读取到这段数据以后通过一个特殊的字体文件（该文件可以根据请求的不同即时更换），将 231 转换成 123。因为爬虫大多没有视图显示，这种加密数据可以一定程度上干扰爬虫的信息获取。当然也可以通过 JS 进行算法转换。但是如果 JS 被拷贝走。那么这个转换很容易就会被破解。使用前端 node 层进行转换也可实现同样的效果，但是本质上原理都是一样的。
2. 使用 canvas，或者是图片。来展示希望隐藏的数据

### 对于前端使用了 node + next 技术方案的站点

可以通过 ip 地址访问次数，或者 userid 的访问次数，来进行屏蔽。因为使用的是服务器端渲染。一旦发现同一个 ip 或者 userID 多次的访问可以直接返回空的页面或者空的数据。

但是实际上对于前端而言并不推荐使用这样的方案，除非有较强的 node + 数据库 + 运维 能力。否则可能造成在正常峰值访问时网站崩溃。

## 速度敏感型爬虫

对于此类爬虫，前端能做的往往比较少。因为要实现每秒成千上万次的爬虫请求不是小个体或者技术背景不强的个体可以完成的。所以此类爬虫较难防范。
